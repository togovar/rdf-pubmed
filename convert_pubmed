#!/bin/bash

##
# MEDLINEのダウンロードとRDF変換の実行 
##

# プロパティ
WORK=/work
DATA=/data
BASE_URL=https://ftp.ncbi.nlm.nih.gov/pubmed/baseline
UPDATE_URL=https://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles
FORCE_CONVERT=0		# 強制実行フラグ
FIRST_RUN=0		# 初回実行フラグ
NUM_PARALLEL=1		# 並列実行数

# オプションの読み取り
while getopts fP: OPT
do
  case $OPT in
    f)  FORCE_CONVERT=1
        ;;
    P)  NUM_PARALLEL=$OPTARG
        ;;
  esac
done
shift  $(($OPTIND - 1))

# ディレクトリの作成
mkdir -p $WORK/baseline

# baselineの中身が空であれば初回実行フラグを立てる
if [ -z "$(ls $WORK/baseline)" ]; then
  FIRST_RUN=1 
fi

# baselineのダウンロード
cd $WORK/baseline

now=`date "+%Y%m%d-%H%M%S"`
echo "Started wget baseline at $now"
wget --execute robots=off -r -l 1 -nd -N -A xml.gz $BASE_URL 2> /baseline_wget.log
now=`date "+%Y%m%d-%H%M%S"`
echo "Finished wget baseline at $now"

cat /baseline_wget.log
baseline_num_of_newfiles=`egrep " saved \[+[0-9]+/+[0-9]+\]" /baseline_wget.log | grep -v "'.listing' saved" | wc -l `

# 初回実行ではなく、baselineに更新がある場合前回ファイルを削除する
if [ $FIRST_RUN -eq 0  ] && [ $baseline_num_of_newfiles -gt 1 ]; then
  echo "baslineに更新があった為、前回ファイルを削除" 
  # baselineとupdatefilesを削除
  cd $WORK
  rm -rf $WORK/baseline
  rm -rf $WORK/updatefiles
  
  # baselineをダウンロードしなおす
  mkdir -p $WORK/baseline
  cd $WORK/baseline
  now=`date "+%Y%m%d-%H%M%S"`
  echo "Started wget baseline at $now"
  wget --execute robots=off -r -l 1 -nd -N -A xml.gz $BASE_URL 2> /baseline_wget.log
  now=`date "+%Y%m%d-%H%M%S"`
  echo "Finished wget baseline at $now"
fi

# baselineに更新がある場合、または出力ファイルが存在しない場合に重複したPMID一覧を出力する
if [ $baseline_num_of_newfiles -gt 1 ] || [ ! -e /work/dup_pmid.tsv ]; then
  test -e /work/dup_pmid.tsv && rm /work/dup_pmid.tsv
  # baseline内の重複したPMIDを抜きだし、最新の更新日の値とペアで出力する
  ruby /pubmed_dup_check.rb
fi

# ディレクトリの作成
mkdir -p $WORK/updatefiles

# updatefilesのダウンロード
cd $WORK/updatefiles

now=`date "+%Y%m%d-%H%M%S"`
echo "Started wget updatefiles at $now"
wget  --execute robots=off -r -l 1 -nd -N -A xml.gz $UPDATE_URL 2>/wget.log
now=`date "+%Y%m%d-%H%M%S"`
echo "Finished wget updatefiles at $now"

cat /wget.log > /dev/stdout

num_of_newfiles=`egrep "saved \[+[0-9]+/+[0-9]+\]" /wget.log | grep -v "'.listing' saved" | wc -l `

# アーカイブファイルに更新がなく、fオプションが指定されていない場合はコンバートを実行しない
if [ $num_of_newfiles -le 1 ] && [ $FORCE_CONVERT -eq 0 ]; then
  echo
  echo "No RDF files were generated because no new files were found at the download site."
  exit 0
fi


cd / 

now=`date "+%Y%m%d-%H%M%S"`
echo "Started convert in $NUM_PARALLEL parallels at $now"

# 削除されたPMIDのリスト作成
ls -d $WORK/updatefiles/pubmed*.xml.gz | xargs -P${NUM_PARALLEL} -IFILE ruby pubmed_delete_citation.rb FILE skip_pmids.txt

# 変換の実行(updatefile)
ls -rd $WORK/updatefiles/pubmed*.xml.gz | xargs -P1 -IFILE ruby /pubmed_updatefiles_converter.rb FILE skip_pmids.txt
# 変換の実行(baseline)
ls -rd $WORK/baseline/pubmed*.xml.gz | xargs -P${NUM_PARALLEL} -IFILE ruby /pubmed_baseline_converter.rb FILE skip_pmids.txt $WORK/dup_pmid.tsv

# turtle fileを圧縮する
for turtle in `ls -1 $DATA/pubmed*.ttl` ; do
  echo "gzip $turtle"
  gzip $turtle
done
chmod -R 666 $(ls -d $WORK/**/pubmed*.xml.gz) && chmod -R 666 $(ls -d $DATA/pubmed*.ttl.gz)

now=`date "+%Y%m%d-%H%M%S"`
echo "Finished convert at $now"

